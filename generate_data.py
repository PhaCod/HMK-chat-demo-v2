#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
generate_data.py ‚Äî Export Gold table ‚Üí data/conversations.csv
==============================================================
Ch·∫°y 1 l·∫ßn ƒë·ªÉ export d·ªØ li·ªáu th·ª±c (ƒë√£ anonymize) cho Streamlit Cloud demo.

C√°ch d√πng:
    python generate_data.py                          # t·ª± detect lakehouse path
    python generate_data.py --lakehouse /opt/lakehouse
    python generate_data.py --synthetic              # t·∫°o d·ªØ li·ªáu t·ªïng h·ª£p

Sau khi ch·∫°y xong:  data/conversations.csv  s·∫Ω ƒë∆∞·ª£c t·∫°o.
Commit file n√†y l√™n GitHub ‚Üí deploy Streamlit Cloud.
"""
import argparse
import glob
import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
GOLD_SUBPATH = "gold/ai_unified_v6"
OUTPUT       = Path(__file__).parent / "data" / "conversations.csv"

# C·ªôt lo·∫°i b·ªè (PII ho·∫∑c qu√° n·∫∑ng)
DROP_COLS = [
    "full_conversation",   # PII + n·∫∑ng
    "thread_id",           # internal ID
    "customer_id",         # PII
    "page_id",             # internal
]

# C·ªôt AI c√≥ th·ªÉ kh√¥ng d√πng trong demo (tu·ª≥ ch·ªçn comment l·∫°i)
DEMO_COLS = [
    "conversation_id", "conversation_date", "page_name",
    "message_count",
    "intent_primary", "purchase_stage", "funnel_type",
    "funnel_is_successful",
    "sentiment_overall", "sentiment_score",
    "disc_primary", "generation_cohort", "lifestyle_segment",
    "urgency_level", "trust_level", "price_sensitivity",
    "agent_overall_score", "empathy_score", "agent_closing_skill",
    "predicted_csat", "conversion_probability",
    "competitor_brand", "product_interest",
    "churn_reason",
    "processed_at",
]

# C√°i paths c√≥ th·ªÉ t√¨m lakehouse
POSSIBLE_PATHS = [
    Path(__file__).parent.parent / "chat-analytics-lakehouse" / "lakehouse",
    Path("/opt/lakehouse"),
    Path(os.environ.get("LAKEHOUSE_PATH", "__NONE__")),
]


# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _find_parquet_files(lakehouse_path: Path) -> list[Path]:
    gold_path = lakehouse_path / GOLD_SUBPATH
    if not gold_path.exists():
        return []
    pattern = str(gold_path / "**" / "*.parquet")
    return [
        Path(f) for f in glob.glob(pattern, recursive=True)
        if "_delta_log" not in f
    ]


def _read_gold(lakehouse_path: Path) -> pd.DataFrame | None:
    files = _find_parquet_files(lakehouse_path)
    if not files:
        return None
    print(f"  ‚Üí {len(files)} parquet files t·∫°i {lakehouse_path / GOLD_SUBPATH}")

    dfs = []
    for f in files:
        try:
            dfs.append(pd.read_parquet(f))
        except Exception as e:
            print(f"  ‚ö† Skip {f.name}: {e}")
    if not dfs:
        return None
    return pd.concat(dfs, ignore_index=True)


def _clean(df: pd.DataFrame) -> pd.DataFrame:
    # Dedup by conversation_id
    if "conversation_id" in df.columns:
        df = df.sort_values("processed_at", ascending=False) if "processed_at" in df.columns else df
        df = df.drop_duplicates(subset=["conversation_id"], keep="first")

    # Drop PII
    for col in DROP_COLS:
        if col in df.columns:
            df = df.drop(columns=[col])

    # Keep only demo columns (plus any extras)
    keep = [c for c in DEMO_COLS if c in df.columns]
    extra = [c for c in df.columns if c not in keep]
    if extra:
        print(f"  ‚Ñπ C·ªôt ph·ª• kh√¥ng d√πng trong demo: {extra[:8]}{'...' if len(extra) > 8 else ''}")
    df = df[keep]

    # Parse dates
    if "conversation_date" in df.columns:
        df["conversation_date"] = pd.to_datetime(df["conversation_date"], errors="coerce")

    # Anonymize page_name if needed (gi·ªØ nguy√™n ƒë·ªÉ demo meaningful)

    return df.reset_index(drop=True)


def _add_snippets(df: pd.DataFrame) -> pd.DataFrame:
    """G√°n conversation_snippet t·ª´ templates (kh√¥ng c√≥ PII)."""
    # Import from app.py n·∫øu c·∫ßn
    try:
        sys.path.insert(0, str(Path(__file__).parent))
        from app import _SNIPPETS
        df["conversation_snippet"] = df["intent_primary"].map(
            lambda x: _SNIPPETS.get(str(x), _SNIPPETS.get("hoi_gia", ""))
        )
        print("  ‚úì ƒê√£ g√°n conversation_snippet t·ª´ templates")
    except ImportError:
        print("  ‚ö† Kh√¥ng import ƒë∆∞·ª£c app.py, b·ªè qua conversation_snippet")
    return df


def generate_synthetic() -> pd.DataFrame:
    """Fallback: t·∫°o d·ªØ li·ªáu t·ªïng h·ª£p."""
    sys.path.insert(0, str(Path(__file__).parent))
    from app import _generate_synthetic_data
    df = _generate_synthetic_data(n=350)
    print(f"  ‚úì ƒê√£ t·∫°o {len(df)} rows d·ªØ li·ªáu t·ªïng h·ª£p")
    return df


# ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main():
    parser = argparse.ArgumentParser(description="Export Gold table cho Streamlit Demo")
    parser.add_argument("--lakehouse", type=str, help="ƒê∆∞·ªùng d·∫´n t·ªõi lakehouse root")
    parser.add_argument("--synthetic", action="store_true", help="D√πng d·ªØ li·ªáu t·ªïng h·ª£p thay v√¨ Gold table")
    parser.add_argument("--n", type=int, default=350, help="S·ªë rows n·∫øu d√πng synthetic (default: 350)")
    args = parser.parse_args()

    print("=" * 55)
    print("  Chat Analytics ‚Äî Data Export for Streamlit Demo")
    print("=" * 55)

    OUTPUT.parent.mkdir(parents=True, exist_ok=True)

    df = None

    if args.synthetic:
        print("‚Ñπ Ch·∫ø ƒë·ªô synthetic ƒë∆∞·ª£c ch·ªçn.")
        df = generate_synthetic()

    else:
        # Try explicit path first
        if args.lakehouse:
            paths_to_try = [Path(args.lakehouse)] + POSSIBLE_PATHS
        else:
            paths_to_try = POSSIBLE_PATHS

        for p in paths_to_try:
            if str(p) == "__NONE__" or not p.exists():
                continue
            print(f"üîç Th·ª≠ ƒë·ªçc Gold table t·∫°i: {p}")
            df = _read_gold(p)
            if df is not None and len(df) > 0:
                print(f"  ‚úì ƒê·ªçc ƒë∆∞·ª£c {len(df)} rows")
                break
            else:
                print(f"  ‚úó Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu")

        if df is None or len(df) == 0:
            print("\n‚ö† Kh√¥ng t√¨m th·∫•y Gold table. D√πng d·ªØ li·ªáu t·ªïng h·ª£p...")
            df = generate_synthetic()
        else:
            print("\nüßπ ƒêang l√†m s·∫°ch v√† anonymize...")
            df = _clean(df)
            df = _add_snippets(df)
            print(f"  ‚úì Sau khi clean: {len(df)} rows, {len(df.columns)} c·ªôt")

    # Save
    df.to_csv(OUTPUT, index=False)
    size_kb = OUTPUT.stat().st_size / 1024
    print(f"\n‚úÖ ƒê√£ l∆∞u ‚Üí {OUTPUT}  ({size_kb:.0f} KB, {len(df):,} rows)")
    print("\nB∆∞·ªõc ti·∫øp theo:")
    print("  1. git add data/conversations.csv")
    print("  2. git commit -m 'Add demo data'")
    print("  3. Deploy l√™n Streamlit Cloud:")
    print("     App file  :  streamlit_demo/app.py")
    print("     Branch    :  main")
    print("=" * 55)


if __name__ == "__main__":
    main()
